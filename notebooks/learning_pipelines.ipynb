{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e3ee95-84af-459d-be56-afdcc1b3a943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(os.path.join(current_dir,\"..\"))\n",
    "\n",
    "from utils.helper import set_openai_key, test_openai_api, create_openai_client, count_tiktoken_length\n",
    "\n",
    "print(\"Import successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66e8a0a4-c2ce-4f0b-a92b-67d9ed654097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key set successfully.\n",
      "This is a test. How can I assist you further?\n"
     ]
    }
   ],
   "source": [
    "# Intialize\n",
    "set_openai_key() # Set openai key\n",
    "test_openai_api() # Test openai api\n",
    "client = create_openai_client() # Create openai client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2852dc-5f16-4a9c-9c17-80c0467908cd",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ecb9996-f81f-4cad-acdb-afe79821fb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are a programming expert.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Please answer the following question.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"When encountering an \\\"IEF450I JOB ABEND\\\" message in a mainframe environment, how would you identify the specific abend code and determine the appropriate corrective actions to resolve the issue before rerunning the job?\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Encountering an \\\"IEF450I JOB ABEND\\\" message in a mainframe environment indicates that a job has abnormally ended (abended). To effectively address this issue, you'll need to follow a systematic approach to identify the specific abend code and determine the appropriate corrective actions. Here's a step-by-step guide:\\n\\n### 1. Gather Initial Information\\n- **Job Output Listings**: Start by reviewing the job output, which includes the JES (Job Entry Subsystem) messages. These messages will provide crucial details about the abend.\\n- **Job Log**: Access the job log to see all system messages and outputs generated during the job's execution. This will help you understand what was happening when the abend occurred.\\n\\n### 2. Identify the Specific Abend Code\\n- **Look for Abend Codes**: Within the job log, locate the abend code, which is typically a four-character code (e.g., S0C7, U4038). It should appear in messages close to the \\\"IEF450I\\\" message.\\n- **System Messages**: Check for additional system messages (e.g., IEC, IEA) that might provide more context about the error.\\n\\n### 3. Analyze the Abend Code\\n- **System Abend Codes (S-codes)**: These are system-related issues. For example, S0C7 indicates a data exception error, often due to invalid data in a computation.\\n- **User Abend Codes (U-codes)**: These are application-specific errors. For instance, U4038 might be triggered by an application-level condition.\\n- **Consult Documentation**: Use IBM documentation or your organization's internal resources to look up the meaning of the abend code. This will give you a clear understanding of what caused the error.\\n\\n### 4. Determine Corrective Actions\\n- **Data Issues**: If the abend is due to data errors (e.g., S0C7), review the input data for invalid formats or values. Correct any discrepancies and ensure the data meets the expected format.\\n- **Program Logic Errors**: If the abend is due to a logic error (e.g., infinite loop), review the program code. Debug and test the logic to ensure it handles all cases correctly.\\n- **Resource Limitations**: If the abend is related to resource constraints (e.g., insufficient memory or CPU), adjust the job's resource parameters or optimize the program to use fewer resources.\\n- **System Configuration**: If the issue is with system settings or JCL (Job\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from pipelines.Learning import ErrorAnalysisPipeline\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = ErrorAnalysisPipeline(client)\n",
    "\n",
    "# Read content\n",
    "with open(\"../assets/examples/learning/analysis.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Generate data\n",
    "data = pipeline(content)\n",
    "\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87531e2-d47d-439d-9ff1-f72a3294b011",
   "metadata": {},
   "source": [
    "### Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbb943b0-6aeb-44d2-8cc3-14ea76115961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are a programming expert.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Please answer the following question.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"What are the key steps and tools involved in integrating legacy mainframe systems with modern applications through RESTful APIs, and how can API security and performance be ensured during this process?\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Integrating legacy mainframe systems with modern applications through RESTful APIs is a strategic move for many organizations looking to modernize their IT infrastructure while leveraging existing investments. Here\\u2019s a detailed breakdown of the key steps and tools involved, along with considerations for ensuring API security and performance:\\n\\n### Key Steps for Integration\\n\\n1. **Assessment and Planning:**\\n   - **Evaluate Legacy Systems:** Understand the data structures, business logic, and interfaces of the legacy systems. Identify the components that need integration.\\n   - **Define Objectives:** Clearly outline what the integration aims to achieve, such as data accessibility, improved functionality, or enhanced user experience.\\n\\n2. **API Design:**\\n   - **Identify Data and Services:** Determine which data and services from the mainframe need to be exposed via APIs.\\n   - **Design RESTful APIs:** Develop a RESTful API architecture that aligns with the needs of modern applications, focusing on resource-based interactions.\\n\\n3. **Middleware Selection:**\\n   - **API Gateway:** Choose an API gateway to manage API traffic, enforce policies, and provide security. Popular options include Apigee, AWS API Gateway, and MuleSoft.\\n   - **Integration Platform:** Use an integration platform or middleware that can connect to the mainframe. IBM Integration Bus, MuleSoft, or Apache Camel are common choices.\\n\\n4. **Development:**\\n   - **API Implementation:** Develop APIs using modern programming languages and frameworks that support RESTful services, such as Java Spring Boot or Node.js.\\n   - **Data Transformation:** Implement data transformation logic to convert mainframe data formats (e.g., COBOL copybooks) to JSON or XML.\\n\\n5. **Testing:**\\n   - **Functional Testing:** Ensure the APIs work as expected with the mainframe data and services.\\n   - **Performance Testing:** Conduct load testing to determine the API's ability to handle expected traffic volumes.\\n\\n6. **Deployment:**\\n   - **Environment Setup:** Deploy the APIs in a scalable environment, such as cloud-based platforms or on-premises servers.\\n   - **Versioning and Documentation:** Implement API versioning and provide comprehensive documentation for developers.\\n\\n### Tools for Integration\\n\\n- **API Management Tools:** Tools like Apigee, AWS API Gateway, and MuleSoft Anypoint Platform facilitate API management, monitoring, and security.\\n- **Middleware Solutions:** IBM Integration Bus, MuleSoft, or Apache Camel provide robust integration capabilities with mainframe systems.\\n- **Data Transformation Tools:** Tools like DataWeave (part of MuleSoft) or custom ETL scripts can handle data\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from pipelines.Learning import IntegrationPipeline\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = IntegrationPipeline(client)\n",
    "\n",
    "# Read content\n",
    "with open(\"../assets/examples/learning/integration.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Generate data\n",
    "data = pipeline(content)\n",
    "\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f875b0a-178b-4491-a2b4-2b2a463ab226",
   "metadata": {},
   "source": [
    "### Performance Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97694d04-5c75-4f68-a499-04ee872eaa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are a programming expert.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Please answer the following question.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"What performance optimization strategies can be implemented to improve the efficiency of batch jobs on a mainframe system, considering factors like job scheduling, resource contention, and data processing requirements?\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Optimizing batch jobs on a mainframe system involves a comprehensive approach that addresses scheduling, resource management, and data handling. Here\\u2019s a detailed breakdown of strategies you can implement:\\n\\n### 1. **Job Scheduling Optimization**\\n\\n- **Prioritization and Dependency Management**: Ensure that critical jobs are prioritized and dependencies are clearly defined. Use job scheduling software to automate this process, ensuring that jobs run in the most efficient order.\\n\\n- **Off-Peak Scheduling**: Schedule non-urgent batch jobs during off-peak hours to minimize contention with online transaction processing (OLTP) systems. This reduces competition for resources and can lead to faster job completion.\\n\\n- **Load Balancing**: Distribute jobs evenly across available resources to prevent bottlenecks. This can be managed through dynamic workload balancing tools that adjust job distribution based on current system load.\\n\\n### 2. **Resource Contention Mitigation**\\n\\n- **Resource Allocation**: Use workload management tools to allocate CPU, memory, and I/O resources dynamically based on job requirements. This ensures that critical jobs receive the necessary resources without over-provisioning.\\n\\n- **Parallel Processing**: Where possible, redesign batch jobs to run in parallel rather than sequentially. This approach can significantly reduce execution time by taking advantage of multiple processors.\\n\\n- **Limit Resource-Intensive Jobs**: Identify jobs that are particularly resource-intensive and analyze if they can be optimized or broken down into smaller, more manageable tasks.\\n\\n### 3. **Data Processing Efficiency**\\n\\n- **Data Filtering**: Implement data filtering at the earliest possible stage. By processing only the necessary data, you can reduce the amount of data that needs to be handled, which speeds up job execution.\\n\\n- **Indexing and Partitioning**: Use database indexing and partitioning strategies to improve data retrieval speeds. Proper indexing can drastically reduce the time spent on data access.\\n\\n- **Efficient I/O Operations**: Optimize I/O operations by using high-performance storage solutions and minimizing unnecessary data transfers. Techniques such as data caching and buffering can also help reduce I/O wait times.\\n\\n### 4. **Code and Algorithm Optimization**\\n\\n- **Review and Refactor Code**: Regularly review batch job code to identify inefficiencies. Refactor code to eliminate redundant operations and improve algorithm efficiency.\\n\\n- **Use Efficient Algorithms**: Choose algorithms that are well-suited to the task at hand. Sometimes, a more complex algorithm may be faster than a simpler one for large data sets.\\n\\n### 5. **Monitoring and Feedback**\\n\\n- **Performance Monitoring Tools**:\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from pipelines.Learning import PerformanceOptimizationPipeline\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = PerformanceOptimizationPipeline(client)\n",
    "\n",
    "# Read content\n",
    "with open(\"../assets/examples/learning/optimization.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Generate data\n",
    "data = pipeline(content)\n",
    "\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd67c70-96b9-4ae2-8c30-de53b97d68bb",
   "metadata": {},
   "source": [
    "### Migration and Modernization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5286a1ec-ad2d-4899-9e73-d539cf7e0534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are a programming expert.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Please answer the following question.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"What are the key steps and considerations involved in migrating an application from a mainframe environment to a cloud platform, and how do these steps ensure compatibility and performance in the new environment?\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Migrating an application from a mainframe environment to a cloud platform is a complex process that involves several key steps and considerations to ensure compatibility, performance, and reliability in the new environment. Here\\u2019s a detailed breakdown of the process:\\n\\n### Key Steps:\\n\\n1. **Assessment and Planning:**\\n   - **Inventory and Analysis:** Begin by conducting a comprehensive inventory of the existing mainframe applications, data, and dependencies. Analyze the architecture, technology stack, and business processes involved.\\n   - **Feasibility Study:** Evaluate whether all or part of the application can be migrated to the cloud. Consider factors such as technical feasibility, cost, and business value.\\n   - **Define Objectives:** Clearly define the goals of the migration, such as cost reduction, scalability, improved performance, or enhanced functionality.\\n\\n2. **Choosing the Right Cloud Strategy:**\\n   - **Rehosting (Lift and Shift):** This involves moving applications to the cloud with minimal changes. It\\u2019s faster but may not leverage cloud-native benefits.\\n   - **Replatforming:** This involves making some optimizations to take advantage of cloud capabilities without changing the core architecture.\\n   - **Refactoring:** This involves re-architecting the application to be cloud-native, which can maximize performance and scalability but requires more time and resources.\\n\\n3. **Selecting the Cloud Provider and Services:**\\n   - Choose a cloud provider that meets your technical and business requirements. Consider factors such as service offerings, pricing, compliance, and support.\\n   - Decide on the cloud services (e.g., compute, storage, database) that best align with your application needs.\\n\\n4. **Data Migration:**\\n   - **Data Assessment:** Evaluate the types of data, volume, and data dependencies. Plan for data transformation and cleansing if necessary.\\n   - **Data Transfer:** Choose the appropriate data transfer method (e.g., batch processing, real-time streaming) based on the data volume and required speed.\\n   - **Data Validation:** Ensure data integrity and consistency post-migration through validation processes.\\n\\n5. **Application Migration:**\\n   - **Code and Architecture Changes:** Modify the application code to ensure compatibility with the cloud environment. This may involve changes in programming languages, libraries, and APIs.\\n   - **Testing:** Conduct thorough testing (unit, integration, performance, and user acceptance testing) to ensure the application functions correctly in the cloud.\\n   - **Optimization:** Optimize the application for cloud performance, including scaling, load balancing, and resource allocation.\\n\\n6. **Security and Compliance:**\\n   - Implement robust security\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from pipelines.Learning import MigrationPipeline\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = MigrationPipeline(client)\n",
    "\n",
    "# Read content\n",
    "with open(\"../assets/examples/learning/migration.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Generate data\n",
    "data = pipeline(content)\n",
    "\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc14a093-ba9b-496e-b9ee-a39de06983a4",
   "metadata": {},
   "source": [
    "### System Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eee7029-9407-4dd7-9811-4de49c872846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are a programming expert.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Please answer the following question.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"What are the key considerations and steps involved in defining and configuring a new LPAR in a mainframe environment to ensure optimal allocation of resources and successful integration into the existing system?\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Defining and configuring a new Logical Partition (LPAR) in a mainframe environment is a critical task that requires careful planning and execution to ensure optimal resource allocation and seamless integration into the existing system. Here are the key considerations and steps involved in this process:\\n\\n### Key Considerations\\n\\n1. **Resource Requirements**:\\n   - **CPU**: Determine the number of processors (CPUs) needed based on the workload requirements. Consider both dedicated and shared processor options.\\n   - **Memory**: Assess the memory needs to support the applications and services running on the LPAR. Ensure there's enough memory to avoid paging and performance degradation.\\n   - **I/O Channels**: Evaluate the input/output requirements, including disk storage and network connectivity, to ensure sufficient bandwidth and low latency.\\n\\n2. **Workload Characteristics**:\\n   - Analyze the expected workload types (e.g., batch processing, online transactions) and their resource consumption patterns to allocate resources appropriately.\\n\\n3. **Scalability and Flexibility**:\\n   - Plan for future growth by considering potential increases in workload and resource needs. Ensure the LPAR can be easily scaled up or down.\\n\\n4. **Security and Isolation**:\\n   - Ensure that the LPAR configuration adheres to security policies and provides adequate isolation from other LPARs to prevent unauthorized access and data breaches.\\n\\n5. **Compatibility and Integration**:\\n   - Verify that the new LPAR is compatible with existing hardware, software, and network configurations. Consider dependencies and integration points with other systems.\\n\\n6. **Redundancy and High Availability**:\\n   - Plan for redundancy and failover capabilities to ensure high availability and minimize downtime in case of hardware failures or maintenance activities.\\n\\n7. **Compliance and Governance**:\\n   - Ensure that the LPAR configuration complies with organizational policies, industry standards, and regulatory requirements.\\n\\n### Steps to Define and Configure a New LPAR\\n\\n1. **Planning and Design**:\\n   - Conduct a thorough assessment of the current environment and future needs.\\n   - Design the LPAR layout, considering resource allocation, network configuration, and security settings.\\n\\n2. **Hardware Configuration**:\\n   - Use the Hardware Management Console (HMC) to define the new LPAR. Specify processor, memory, and I/O resources.\\n   - Set up the necessary physical and virtual network interfaces.\\n\\n3. **Software Installation and Configuration**:\\n   - Install the operating system and any required software on the LPAR.\\n   - Configure system parameters, such as time zone, language, and network settings.\\n\\n4. **\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from pipelines.Learning import SystemConfigurationPipeline\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = SystemConfigurationPipeline(client)\n",
    "\n",
    "# Read content\n",
    "with open(\"../assets/examples/learning/configuration.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Generate data\n",
    "data = pipeline(content)\n",
    "\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1616a60c-36e4-4ef5-b08b-25a0daa584b9",
   "metadata": {},
   "source": [
    "### Programming on Mainframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f59b3f1-4920-4bc4-9743-e48454975e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are a programming expert.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Please answer the following question.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Write a COBOL program that reads employee data from a VSAM file, calculates the total salary of all employees, and displays the total salary. The VSAM file contains records with fields for employee ID (5 characters), name (30 characters), and salary (9 digits with 2 decimal places). Ensure your program handles file opening, reading, and error checking appropriately.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"To create a COBOL program that reads employee data from a VSAM file, calculates the total salary of all employees, and displays the total salary, we need to follow a structured approach. The program will include sections for file handling, data processing, and error checking. Below is a detailed explanation and the corresponding COBOL code.\\n\\n### COBOL Program Structure\\n\\n1. **IDENTIFICATION DIVISION**: This section contains the program's name and other metadata.\\n2. **ENVIRONMENT DIVISION**: This section specifies the configuration of the input and output files, including the VSAM file.\\n3. **DATA DIVISION**: This section defines the data structures used in the program, including the file record layout and working storage variables.\\n4. **PROCEDURE DIVISION**: This section contains the logic for opening the file, reading records, calculating the total salary, and handling errors.\\n\\n### COBOL Code\\n\\n```cobol\\nIDENTIFICATION DIVISION.\\nPROGRAM-ID. CalculateTotalSalary.\\n\\nENVIRONMENT DIVISION.\\nINPUT-OUTPUT SECTION.\\nFILE-CONTROL.\\n    SELECT EmployeeFile ASSIGN TO 'EMPLOYEE.VSAM'\\n        ORGANIZATION IS INDEXED\\n        ACCESS MODE IS SEQUENTIAL\\n        RECORD KEY IS EmployeeID\\n        FILE STATUS IS FileStatus.\\n\\nDATA DIVISION.\\nFILE SECTION.\\nFD EmployeeFile.\\n01 EmployeeRecord.\\n   05 EmployeeID      PIC X(5).\\n   05 EmployeeName    PIC X(30).\\n   05 EmployeeSalary  PIC 9(7)V99.\\n\\nWORKING-STORAGE SECTION.\\n01 WS-Totals.\\n   05 TotalSalary     PIC 9(11)V99 VALUE 0.\\n\\n01 WS-FileStatus.\\n   05 FileStatus      PIC XX.\\n   88 EndOfFile       VALUE '10'.\\n   88 FileOpenError   VALUE '90'.\\n\\nPROCEDURE DIVISION.\\nBegin.\\n    OPEN INPUT EmployeeFile\\n    IF FileStatus = '00'\\n        PERFORM ReadAndCalculate\\n    ELSE\\n        DISPLAY \\\"Error opening file. Status: \\\" FileStatus\\n    END-IF\\n    CLOSE EmployeeFile\\n    DISPLAY \\\"Total Salary of all Employees: \\\" TotalSalary\\n    STOP RUN.\\n\\nReadAndCalculate.\\n    PERFORM UNTIL EndOfFile\\n        READ EmployeeFile INTO EmployeeRecord\\n            AT END\\n                SET EndOfFile TO TRUE\\n            NOT AT END\\n                ADD EmployeeSalary TO TotalSalary\\n        END-READ\\n    END-PERFORM.\\n```\\n\\n### Explanation of the Code\\n\\n\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from pipelines.Learning import ProgrammingPipeline\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = ProgrammingPipeline(client)\n",
    "\n",
    "# Read content\n",
    "with open(\"../assets/examples/learning/programming.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Generate data\n",
    "data = pipeline(content)\n",
    "\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c8b540-5541-4f29-a11e-d9821eaae519",
   "metadata": {},
   "source": [
    "### System Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dd4d020-452f-441b-8e08-abd9e6e1c9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are a programming expert.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Please answer the following question.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"What are the critical steps involved in performing an Initial Program Load (IPL) on an IBM mainframe to ensure a successful system startup?\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Performing an Initial Program Load (IPL) on an IBM mainframe is a crucial process to bring the system from a powered-off state to a fully operational state. The IPL process is similar to booting up a computer, but given the complexity and critical nature of mainframe systems, it involves several important steps. Here\\u2019s a detailed look at the critical steps involved in performing an IPL:\\n\\n1. **Preparation and Planning**:\\n   - **Review System Documentation**: Before initiating an IPL, ensure you understand the specific configurations and requirements of your mainframe system. This includes hardware configurations, system parameters, and any recent changes or updates.\\n   - **Determine the Type of IPL**: Decide whether a cold start (initializing everything from scratch) or a warm start (preserving certain system states) is needed. This depends on the circumstances, such as a planned maintenance IPL versus a recovery IPL after a system failure.\\n\\n2. **Notify Stakeholders**:\\n   - Inform relevant personnel about the planned IPL, including system administrators, application owners, and users. This helps minimize disruptions and ensures that everyone is prepared for potential downtime.\\n\\n3. **Backup Critical Data**:\\n   - Ensure that all critical data and system configurations are backed up. This is essential in case something goes wrong during the IPL process and a rollback is needed.\\n\\n4. **Configure the Hardware Management Console (HMC)**:\\n   - Access the HMC to manage the mainframe\\u2019s hardware settings. The HMC is used to control the power status of the mainframe and to initiate the IPL process.\\n   - Verify that all hardware components are in a ready state and check for any hardware alerts or issues that need to be addressed before proceeding.\\n\\n5. **Select the Correct Load Parameters**:\\n   - On the HMC, select the appropriate load profile or parameters. This includes specifying the correct I/O configuration and selecting the proper system dataset to be loaded (commonly referred to as the SYSRES).\\n\\n6. **Initialize the IPL Process**:\\n   - Start the IPL from the HMC by selecting the IPL option and confirming the action. The system will begin loading the initial program from the specified dataset.\\n\\n7. **Monitor System Messages and Logs**:\\n   - As the IPL progresses, closely monitor system messages and logs. The system console will display various messages indicating the status of the IPL process. Pay attention to any error messages or warnings that may require intervention.\\n\\n8. **Load and Verify System Components**:\\n   - The system will load the operating system and initialize critical system\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from pipelines.Learning import SystemOperationPipeline\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = SystemOperationPipeline(client)\n",
    "\n",
    "# Read content\n",
    "with open(\"../assets/examples/learning/operation.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Generate data\n",
    "data = pipeline(content)\n",
    "\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d432400-33aa-4c91-9cbf-d9dc953059cc",
   "metadata": {},
   "source": [
    "### Command References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a1b1015-eacb-4147-a11c-e8fa762ce794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are a programming expert.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Please answer the following question.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"What z/OS command would you use to list all currently active jobs on the mainframe for monitoring system performance and resource usage?\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"To list all currently active jobs on a z/OS mainframe, you would typically use the System Display and Search Facility (SDSF), which is an interactive interface that provides a way to monitor and manage jobs, output, and resources on the system. However, if you're looking for a command-line approach, you can use the `DISPLAY` command from the system console to get information about active jobs.\\n\\nHere's a commonly used command to achieve this:\\n\\n```plaintext\\nD A,L\\n```\\n\\nThis command stands for \\\"Display Active, Long.\\\" It will provide a list of all active address spaces, which includes batch jobs, started tasks, and system address spaces. The output will show you the job name, job number, and other relevant details, which can be useful for monitoring system performance and resource usage.\\n\\nIf you are using SDSF, you can also enter the `DA` (Display Active) command from within SDSF to view active jobs. This interface allows you to sort and filter the job list based on various criteria, making it easier to monitor specific jobs or job types.\\n\\nAdditionally, if you have access to more advanced monitoring tools like IBM OMEGAMON or similar, these can provide even more detailed insights into system performance and resource usage.\\n\\nOverall, the choice of method may depend on your specific environment and the tools available to you.\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from pipelines.Learning import CommandReferencePipeline\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = CommandReferencePipeline(client)\n",
    "\n",
    "# Read content\n",
    "with open(\"../assets/examples/learning/reference.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Generate data\n",
    "data = pipeline(content)\n",
    "\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154cb9aa-25f1-4deb-a08f-dbf7d674d587",
   "metadata": {},
   "source": [
    "### Troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e47ca314-ab3e-45f3-974e-4c44fe97ccde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are a programming expert.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Please answer the following question.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"What steps would you take to identify and resolve high CPU usage on an IBM z/OS mainframe system using RMF reports, WLM policies, and system logs?\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Troubleshooting high CPU usage on an IBM z/OS mainframe system involves a systematic approach using various tools and techniques. Here\\u2019s a detailed step-by-step guide to help you identify and resolve the issue:\\n\\n### Step 1: Initial Assessment\\n1. **Understand the Symptoms**: Gather information about the high CPU usage, such as when it started, any changes made to the system recently, and the impact on applications and users.\\n2. **Check System Logs**: Review system logs for any errors or warnings that might indicate what processes are consuming more CPU than expected.\\n\\n### Step 2: Use RMF (Resource Measurement Facility) Reports\\n1. **Collect RMF Data**: Use RMF to collect performance data. RMF provides detailed reports on system performance, including CPU utilization.\\n2. **Analyze CPU Activity**: Look at the RMF CPU Activity report. This will show you which workloads are consuming the most CPU. Pay attention to the LPAR (Logical Partition) utilization and the CPU busy percentage.\\n3. **Identify Peak Usage Times**: Determine if high CPU usage occurs during specific times or if it is consistent. This can help identify batch jobs or specific processes that might be causing the issue.\\n\\n### Step 3: Review WLM (Workload Manager) Policies\\n1. **Check WLM Policies**: Ensure that WLM policies are correctly set up. WLM manages system resources and prioritizes workloads, so incorrect settings can lead to suboptimal CPU usage.\\n2. **Analyze Service Class Goals**: Review the service class goals and ensure they align with business priorities. If a low-priority workload is consuming too much CPU, consider adjusting its importance or goals.\\n3. **Adjust Resource Allocation**: If necessary, adjust the resource allocation in WLM to ensure critical workloads have the CPU resources they need.\\n\\n### Step 4: Drill Down into Specific Workloads\\n1. **Identify Problematic Jobs or Tasks**: Use RMF reports to identify specific jobs or tasks that are consuming excessive CPU. Look for any long-running or looping jobs.\\n2. **Investigate Application Code**: If a particular application is identified as the culprit, review its code for inefficiencies. This might involve working with developers to optimize algorithms or queries.\\n3. **Check for Recent Changes**: Determine if any recent changes to applications or system configurations might have led to increased CPU usage.\\n\\n### Step 5: System Configuration and Optimization\\n1. **Review System Configuration**: Ensure that the system is configured optimally for\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from pipelines.Learning import TroubleshootingPipeline\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = TroubleshootingPipeline(client)\n",
    "\n",
    "# Read content\n",
    "with open(\"../assets/examples/learning/troubleshooting.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Generate data\n",
    "data = pipeline(content)\n",
    "\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8988ee29-74be-4c32-adda-dce61a7cf4e3",
   "metadata": {},
   "source": [
    "### Definitions and Terminology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bfbbd51-341a-457f-a64d-ccdf505168a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are a programming expert.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Please answer the following question.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"What are the key characteristics that define mainframe systems, and in which industries are they typically used?\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Mainframe systems are known for their reliability, scalability, and ability to handle massive volumes of data and transactions. Here are some key characteristics that define them:\\n\\n1. **High Performance and Throughput**: Mainframes are designed to process large amounts of data quickly and efficiently. They can handle millions of transactions per second, which is crucial for industries that require real-time processing.\\n\\n2. **Reliability and Availability**: These systems are built to be highly reliable and are often used in environments where downtime is not an option. They typically offer features like redundant components and failover capabilities to ensure continuous operation.\\n\\n3. **Scalability**: Mainframes can scale both vertically and horizontally, meaning they can handle increasing workloads by adding more resources or by upgrading existing ones.\\n\\n4. **Security**: Mainframes offer robust security features, including advanced encryption and access controls, making them suitable for industries that handle sensitive data.\\n\\n5. **Virtualization**: Mainframes can run multiple operating systems and applications concurrently, which allows for efficient resource utilization and flexibility.\\n\\n6. **Legacy System Support**: Many mainframes are used to run legacy applications that are critical to business operations, providing a stable environment for these older systems.\\n\\nAs for the industries where mainframes are typically used, you'll find them in sectors that require high-volume transaction processing and data management:\\n\\n- **Financial Services**: Banks and insurance companies use mainframes for processing transactions, managing accounts, and running complex financial models.\\n- **Healthcare**: Hospitals and healthcare providers rely on mainframes to manage patient records, billing systems, and other critical data.\\n- **Government**: Various government agencies use mainframes for data processing, tax systems, and social services management.\\n- **Retail**: Large retailers use mainframes for inventory management, point-of-sale systems, and customer data processing.\\n- **Telecommunications**: Mainframes help manage billing systems, customer databases, and network operations for telecom companies.\\n\\nOverall, mainframes are the backbone of many critical operations across these industries, ensuring that data is processed efficiently and securely.\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from pipelines.Learning import DefinitionPipeline\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = DefinitionPipeline(client)\n",
    "\n",
    "# Read content\n",
    "with open(\"../assets/examples/learning/definition.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Generate data\n",
    "data = pipeline(content)\n",
    "\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2ae7c0-07fc-4a8b-9b50-654438584fea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
