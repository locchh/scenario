{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e3ee95-84af-459d-be56-afdcc1b3a943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(os.path.join(current_dir,\"..\"))\n",
    "\n",
    "from utils.helper import set_openai_key, test_openai_api, create_openai_client, count_tiktoken_length\n",
    "\n",
    "print(\"Import successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66e8a0a4-c2ce-4f0b-a92b-67d9ed654097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key set successfully.\n",
      "This is a test.\n"
     ]
    }
   ],
   "source": [
    "# Intialize\n",
    "set_openai_key() # Set openai key\n",
    "test_openai_api() # Test openai api\n",
    "client = create_openai_client() # Create openai client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2852dc-5f16-4a9c-9c17-80c0467908cd",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ecb9996-f81f-4cad-acdb-afe79821fb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are a programming expert.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Please answer the following question.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"What steps should a system administrator take to identify and resolve the cause of an \\\"IEF450I JOB ABEND\\\" error in a mainframe environment, and how can specific abend codes like S222 or S806 assist in this process?\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"When a system administrator encounters an \\\"IEF450I JOB ABEND\\\" error in a mainframe environment, it indicates that a job has abnormally ended. To effectively identify and resolve the cause of this error, the administrator should follow a structured approach:\\n\\n### Step 1: Gather Information\\n- **Review the Job Log:** Start by examining the job log and output messages. The \\\"IEF450I\\\" message provides context about the job that abended, including the job name, job ID, and the specific abend code.\\n- **Identify the Abend Code:** The abend code, such as S222 or S806, is crucial for diagnosing the problem. Each code has a specific meaning and can guide the troubleshooting process.\\n\\n### Step 2: Understand the Specific Abend Code\\n- **S222 Abend Code:** This code indicates that the job was cancelled by a user or system operator. It often occurs when a job exceeds its time limit or is manually terminated. To address this:\\n  - Check if the job was intentionally cancelled and why.\\n  - Review the job's resource usage and execution time to see if it needs optimization or adjustment in its time limits.\\n  - Consult with the operator or user who may have cancelled the job for more insights.\\n\\n- **S806 Abend Code:** This code signifies a program not found error, meaning the system couldn't locate a required program or module. To resolve this:\\n  - Verify that the program name is correctly specified in the JCL (Job Control Language).\\n  - Ensure that the necessary program libraries are correctly included in the job's STEPLIB or JOBLIB.\\n  - Check if the program has been deleted, moved, or if there are permission issues preventing access.\\n\\n### Step 3: Investigate and Resolve\\n- **Check System Resources and Configuration:** Ensure that all system resources such as memory, CPU, and disk space are sufficient and properly configured for the job's needs.\\n- **Review Recent Changes:** Consider any recent changes to the system or application configurations that might have impacted the job.\\n- **Consult Documentation:** Use IBM documentation or internal company resources to understand the specific abend codes and suggested resolutions.\\n- **Engage with the Development Team:** If the issue relates to application logic or code, work with developers to identify and fix any underlying problems.\\n\\n### Step 4: Test and Validate\\n- **Rerun the Job:** Once adjustments are made, rerun the job to verify that the issue is resolved.\\n- **Monitor the Job Execution:** Keep an eye on the job's\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from pipelines.Learning import ErrorAnalysisPipeline\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = ErrorAnalysisPipeline(client)\n",
    "\n",
    "# Read content\n",
    "with open(\"../assets/examples/learning/analysis.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Generate data\n",
    "data = pipeline(content)\n",
    "\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87531e2-d47d-439d-9ff1-f72a3294b011",
   "metadata": {},
   "source": [
    "### Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbb943b0-6aeb-44d2-8cc3-14ea76115961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are a programming expert.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Please answer the following question.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"What are some key techniques and tools used to integrate legacy mainframe systems with modern applications via RESTful APIs, and how can these integrations be tested and secured?\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Integrating legacy mainframe systems with modern applications via RESTful APIs is a complex task that requires a careful approach to ensure seamless communication, maintain data integrity, and secure sensitive information. Here are some key techniques and tools, along with methods for testing and securing these integrations:\\n\\n### Techniques for Integration\\n\\n1. **API Wrappers**: \\n   - **Description**: Create an API wrapper around the legacy system to expose its functionalities as RESTful services. This involves developing a middleware layer that translates RESTful requests into a format the mainframe understands and vice versa.\\n   - **Tools**: Tools like IBM z/OS Connect and Apigee can help in creating these wrappers efficiently.\\n\\n2. **Message Queues**:\\n   - **Description**: Use message queuing systems like IBM MQ or RabbitMQ to facilitate asynchronous communication between the mainframe and modern applications. This is useful for handling high-volume transactions and decoupling systems.\\n   - **Tools**: IBM MQ, RabbitMQ, Apache Kafka.\\n\\n3. **Data Virtualization**:\\n   - **Description**: This technique involves creating a virtual data layer that provides a unified view of data from multiple sources, including mainframes, without moving the data. This can be accessed via APIs.\\n   - **Tools**: Denodo, TIBCO Data Virtualization.\\n\\n4. **ETL Processes**:\\n   - **Description**: Extract, Transform, Load (ETL) processes can be used to periodically sync data between the mainframe and modern systems, enabling RESTful APIs to access up-to-date information.\\n   - **Tools**: Informatica, Talend, Apache Nifi.\\n\\n5. **Microservices Architecture**:\\n   - **Description**: Break down monolithic legacy functionalities into microservices that can be accessed via APIs. This can involve re-engineering parts of the legacy system.\\n   - **Tools**: Spring Boot, Docker, Kubernetes.\\n\\n### Testing Integrations\\n\\n1. **Unit and Integration Testing**:\\n   - **Description**: Develop test cases that validate the functionality of individual API endpoints and the interaction between different components.\\n   - **Tools**: JUnit, Postman, SoapUI.\\n\\n2. **Performance Testing**:\\n   - **Description**: Ensure the system can handle expected load and stress conditions. Test the response times and throughput of API calls.\\n   - **Tools**: Apache JMeter, LoadRunner.\\n\\n3. **End-to-End Testing**:\\n   - **Description**: Simulate real-world scenarios to ensure\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from pipelines.Learning import IntegrationPipeline\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = IntegrationPipeline(client)\n",
    "\n",
    "# Read content\n",
    "with open(\"../assets/examples/learning/integration.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Generate data\n",
    "data = pipeline(content)\n",
    "\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f875b0a-178b-4491-a2b4-2b2a463ab226",
   "metadata": {},
   "source": [
    "### Performance Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97694d04-5c75-4f68-a499-04ee872eaa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are a programming expert.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Please answer the following question.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"What are some effective strategies for optimizing batch job performance on a mainframe system, and how can these strategies be applied to minimize resource contention and improve processing efficiency?\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Optimizing batch job performance on a mainframe system involves several strategies that focus on minimizing resource contention, improving processing efficiency, and ensuring that jobs complete within the required timeframes. Here are some effective strategies and how they can be applied:\\n\\n1. **Job Scheduling and Prioritization**:\\n   - **Job Scheduling Tools**: Utilize advanced scheduling tools like IBM's Workload Scheduler or CA-7 to manage job dependencies, priorities, and timing. These tools help in spreading the workload evenly across the available time window and can automatically adjust schedules based on resource availability.\\n   - **Prioritize Critical Jobs**: Assign higher priority to critical jobs that need to complete within a specific time. This ensures that essential processes receive the necessary resources and processing time.\\n\\n2. **Resource Management**:\\n   - **CPU and Memory Allocation**: Use mainframe capabilities to allocate CPU and memory resources dynamically. This involves setting appropriate limits and priorities for batch jobs to ensure that no single job monopolizes resources.\\n   - **I/O Optimization**: Reduce I/O contention by organizing datasets to minimize access times. Techniques include using larger block sizes, buffering, and caching frequently accessed data.\\n   - **Parallel Processing**: Break down large batch jobs into smaller, parallel tasks that can run simultaneously, taking advantage of multi-processor systems to reduce overall execution time.\\n\\n3. **Code Optimization**:\\n   - **Efficient Coding Practices**: Ensure that batch programs are written efficiently. This includes optimizing algorithms, using efficient data structures, and minimizing unnecessary computations.\\n   - **Review and Refactor Legacy Code**: Regularly review and refactor legacy code to remove inefficiencies and apply modern programming practices that can enhance performance.\\n\\n4. **Data Management**:\\n   - **Data Archiving and Purging**: Regularly archive or purge old and unnecessary data to reduce the volume of data processed during batch runs. This reduces I/O operations and speeds up processing.\\n   - **Database Optimization**: Ensure that databases are indexed appropriately and that queries are optimized to reduce processing time. This might involve restructuring database tables or optimizing SQL queries.\\n\\n5. **Monitoring and Analysis**:\\n   - **Performance Monitoring Tools**: Implement monitoring tools to track the performance of batch jobs in real-time. Tools like IBM OMEGAMON or BMC MainView can provide insights into resource usage and identify bottlenecks.\\n   - **Analyze Historical Data**: Use historical performance data to identify trends and patterns that can inform future optimizations. This data can help predict peak usage times and adjust scheduling\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from pipelines.Learning import PerformanceOptimizationPipeline\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = PerformanceOptimizationPipeline(client)\n",
    "\n",
    "# Read content\n",
    "with open(\"../assets/examples/learning/optimization.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Generate data\n",
    "data = pipeline(content)\n",
    "\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd67c70-96b9-4ae2-8c30-de53b97d68bb",
   "metadata": {},
   "source": [
    "### Migration and Modernization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5286a1ec-ad2d-4899-9e73-d539cf7e0534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are a programming expert.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Please answer the following question.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"What are the key steps and considerations involved in migrating an application from a mainframe environment to a cloud platform, including the assessment of application architecture, identification of mainframe-specific components, selection of a cloud platform and migration strategy, data conversion, and post-migration testing and validation?\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Migrating an application from a mainframe environment to a cloud platform is a complex process that involves several key steps and considerations. This process requires careful planning and execution to ensure a successful transition. Below is a detailed response outlining the essential steps and considerations:\\n\\n### 1. Assessment of Application Architecture\\n- **Inventory and Analysis**: Begin by creating a comprehensive inventory of all applications, their dependencies, and the underlying infrastructure. Understand the business processes supported by each application.\\n- **Architecture Evaluation**: Analyze the current architecture to identify components, interfaces, and data flows. Evaluate the application\\u2019s scalability, performance, and security requirements.\\n- **Code Analysis**: Review the application code to understand its complexity, dependencies, and potential compatibility issues with cloud environments.\\n\\n### 2. Identification of Mainframe-Specific Components\\n- **Legacy Technologies**: Identify mainframe-specific technologies such as COBOL, PL/I, CICS, DB2, IMS, and JCL scripts that need special attention during migration.\\n- **Integration Points**: Determine how the application interacts with other systems and services, including batch processing, transaction processing, and data storage.\\n- **Workload Characteristics**: Understand the workload patterns, including peak usage times, transaction volumes, and batch processing schedules.\\n\\n### 3. Selection of a Cloud Platform\\n- **Cloud Provider Evaluation**: Evaluate cloud providers (e.g., AWS, Azure, Google Cloud) based on factors such as service offerings, pricing, compliance, and support for legacy technologies.\\n- **Service Model Selection**: Decide on the appropriate cloud service model (IaaS, PaaS, SaaS) that aligns with your application\\u2019s needs and organizational goals.\\n- **Geographical Considerations**: Consider data residency requirements and latency issues when selecting cloud regions.\\n\\n### 4. Migration Strategy\\n- **Rehosting (Lift-and-Shift)**: Move applications to the cloud with minimal changes. This strategy is quick but may not leverage cloud-native benefits.\\n- **Refactoring**: Modify the application to better fit the cloud environment, which can involve rewriting parts of the code to improve scalability and performance.\\n- **Replatforming**: Make minor changes to optimize the application for the cloud without altering its core architecture.\\n- **Retiring and Replacing**: Decommission outdated applications and replace them with cloud-native solutions if applicable.\\n- **Pilot Testing**: Conduct a pilot migration with a non-critical application or a subset of the application to validate the chosen strategy.\\n\\n### 5. Data Conversion\\n- **Data Assessment\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from pipelines.Learning import MigrationPipeline\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = MigrationPipeline(client)\n",
    "\n",
    "# Read content\n",
    "with open(\"../assets/examples/learning/migration.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Generate data\n",
    "data = pipeline(content)\n",
    "\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc14a093-ba9b-496e-b9ee-a39de06983a4",
   "metadata": {},
   "source": [
    "### System Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eee7029-9407-4dd7-9811-4de49c872846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are a programming expert.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Please answer the following question.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"What are the critical steps involved in configuring a new LPAR on a mainframe system to ensure proper resource allocation and integration, and how do you monitor its successful initialization and operation?\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Configuring a new Logical Partition (LPAR) on a mainframe system is a complex process that involves several critical steps to ensure proper resource allocation and integration. Here\\u2019s a detailed guide to the process:\\n\\n### 1. **Planning and Requirements Gathering**\\n   - **Understand Workload Needs**: Determine the specific workload requirements for the LPAR. This includes CPU, memory, I/O, and storage needs.\\n   - **Resource Availability**: Assess the available resources on the mainframe to ensure they meet the LPAR\\u2019s needs without impacting existing workloads.\\n\\n### 2. **Hardware Management Console (HMC) Configuration**\\n   - **Access the HMC**: Use the HMC to create and manage LPARs. Ensure you have the necessary administrative permissions.\\n   - **Define LPAR Parameters**: Set up basic parameters such as LPAR name, processor type (dedicated or shared), and memory allocation.\\n   - **Processor Configuration**: Decide on the number of logical processors and their type (CP, IFL, zIIP, etc.).\\n\\n### 3. **Resource Allocation**\\n   - **Memory Allocation**: Allocate sufficient memory based on workload analysis. Consider future growth and scalability.\\n   - **I/O Configuration**: Define I/O paths and assign appropriate channel paths, control units, and device addresses.\\n   - **Storage Allocation**: Ensure that DASD (Direct Access Storage Device) volumes are properly configured and accessible.\\n\\n### 4. **Networking Configuration**\\n   - **Virtual Network Setup**: Configure virtual network interfaces and ensure proper IP addressing and VLAN tagging if necessary.\\n   - **External Network Integration**: Ensure connectivity with external networks and systems, setting up necessary routing and security rules.\\n\\n### 5. **Security Configuration**\\n   - **Access Control**: Set up user access controls and permissions for the LPAR.\\n   - **Encryption and Compliance**: Implement necessary encryption standards and compliance measures, such as auditing and logging.\\n\\n### 6. **Operating System Installation**\\n   - **OS Selection and Installation**: Install the appropriate operating system (e.g., z/OS, Linux on Z) on the LPAR.\\n   - **Patches and Updates**: Apply necessary patches and updates to ensure the OS is secure and up-to-date.\\n\\n### 7. **Integration and Testing**\\n   - **Subsystem Configuration**: Configure subsystems such as CICS, IMS, DB2, etc., as required by the LPAR\\u2019s workload.\\n   - **Testing**: Conduct thorough testing to\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from pipelines.Learning import SystemConfigurationPipeline\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = SystemConfigurationPipeline(client)\n",
    "\n",
    "# Read content\n",
    "with open(\"../assets/examples/learning/configuration.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Generate data\n",
    "data = pipeline(content)\n",
    "\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1616a60c-36e4-4ef5-b08b-25a0daa584b9",
   "metadata": {},
   "source": [
    "### Programming on Mainframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f59b3f1-4920-4bc4-9743-e48454975e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are a programming expert.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Please answer the following question.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Write a COBOL program that reads employee data from a VSAM file, processes each record to calculate a 10% bonus on the salary, and then displays the employee ID, name, original salary, and calculated bonus. Ensure your program includes appropriate sections to define the file structure, handle file operations, and implement the logic for reading records and calculating the bonus.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Creating a COBOL program to read employee data from a VSAM file, calculate a 10% bonus on the salary, and display the necessary information involves several steps. Below is a detailed explanation and the corresponding COBOL code to achieve this task.\\n\\n### COBOL Program Structure\\n\\n1. **Identification Division**: This section includes the program name and other metadata.\\n2. **Environment Division**: This section specifies the input-output configuration, including the VSAM file.\\n3. **Data Division**: This section defines the data structures used in the program, including the file layout and working storage variables.\\n4. **Procedure Division**: This section contains the logic for reading the file, calculating the bonus, and displaying the results.\\n\\n### COBOL Code\\n\\n```cobol\\nIDENTIFICATION DIVISION.\\nPROGRAM-ID. EmployeeBonusCalculator.\\n\\nENVIRONMENT DIVISION.\\nINPUT-OUTPUT SECTION.\\nFILE-CONTROL.\\n    SELECT EmployeeFile ASSIGN TO 'EMPLOYEE.VSAM'\\n        ORGANIZATION IS INDEXED\\n        ACCESS MODE IS SEQUENTIAL\\n        RECORD KEY IS EmployeeID\\n        FILE STATUS IS WS-FileStatus.\\n\\nDATA DIVISION.\\nFILE SECTION.\\nFD  EmployeeFile.\\n01  EmployeeRecord.\\n    05  EmployeeID         PIC X(10).\\n    05  EmployeeName       PIC X(30).\\n    05  EmployeeSalary     PIC 9(7)V99.\\n\\nWORKING-STORAGE SECTION.\\n01  WS-FileStatus         PIC X(2) VALUE SPACES.\\n01  WS-Bonus              PIC 9(7)V99 VALUE 0.\\n01  WS-DisplayLine.\\n    05  WS-DisplayID      PIC X(10).\\n    05  WS-DisplayName    PIC X(30).\\n    05  WS-DisplaySalary  PIC 9(7)V99.\\n    05  WS-DisplayBonus   PIC 9(7)V99.\\n\\nPROCEDURE DIVISION.\\nBegin.\\n    OPEN INPUT EmployeeFile\\n    IF WS-FileStatus NOT = \\\"00\\\"\\n        DISPLAY \\\"Error opening file. Status: \\\" WS-FileStatus\\n        STOP RUN\\n    END-IF\\n\\n    PERFORM UNTIL WS-FileStatus = \\\"10\\\"\\n        READ EmployeeFile INTO EmployeeRecord\\n            AT END\\n                MOVE \\\"10\\\" TO WS-FileStatus\\n            NOT AT END\\n                PERFORM Calculate-And-Display\\n        END-READ\\n    END-PERFORM\\n\\n    CLOSE EmployeeFile\\n    DISPLAY \\\"Processing complete.\\\"\\n   \"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from pipelines.Learning import ProgrammingPipeline\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = ProgrammingPipeline(client)\n",
    "\n",
    "# Read content\n",
    "with open(\"../assets/examples/learning/programming.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Generate data\n",
    "data = pipeline(content)\n",
    "\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c8b540-5541-4f29-a11e-d9821eaae519",
   "metadata": {},
   "source": [
    "### System Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dd4d020-452f-441b-8e08-abd9e6e1c9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are a programming expert.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Please answer the following question.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"What are the key steps involved in performing an Initial Program Load (IPL) on an IBM mainframe, and why is it important to ensure all critical jobs are stopped before initiating this process?\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Performing an Initial Program Load (IPL) on an IBM mainframe is a critical operation that essentially involves restarting the system. This process is akin to rebooting a computer but is far more complex due to the nature and scale of mainframe operations. Here are the key steps involved in performing an IPL and the reasons why it's crucial to ensure all critical jobs are stopped beforehand:\\n\\n### Key Steps in Performing an IPL\\n\\n1. **Preparation and Planning:**\\n   - **Review Documentation:** Before initiating an IPL, review system documentation and checklists to ensure all necessary steps and dependencies are accounted for.\\n   - **Notify Stakeholders:** Inform all relevant stakeholders, including system operators, application owners, and end-users, about the planned IPL. This ensures that everyone is aware of potential downtime and can plan accordingly.\\n\\n2. **Backup and Data Integrity:**\\n   - **Data Backup:** Ensure that all critical data and system configurations are backed up. This is crucial in case something goes wrong during the IPL process.\\n   - **Checkpointing:** Use checkpointing to save the state of critical applications, which can help in recovery if needed.\\n\\n3. **Stop Critical Jobs and Processes:**\\n   - **Graceful Shutdown:** Ensure that all critical jobs, applications, and services are stopped gracefully. This prevents data corruption and ensures that processes can be resumed smoothly after the IPL.\\n   - **Verify Job Completion:** Check that all jobs have completed successfully and that no critical transactions are in progress.\\n\\n4. **System Configuration and Setup:**\\n   - **Load Configuration:** Load the necessary system configuration parameters that will be used during the IPL.\\n   - **Select Load Source:** Choose the appropriate load source, which could be a specific disk or a network location, depending on the system setup.\\n\\n5. **Initiate IPL:**\\n   - **Execute IPL Command:** Use the system console to issue the IPL command. This will start the process of loading the operating system from the specified source.\\n   - **Monitor Progress:** Carefully monitor the IPL process for any errors or warnings. This can involve checking system logs and console messages.\\n\\n6. **Post-IPL Verification:**\\n   - **System Health Check:** Once the IPL is complete, perform a thorough check of the system's health. This includes verifying that all hardware components are functioning correctly and that the operating system is running as expected.\\n   - **Restart Critical Jobs:** Restart all critical jobs and services that were stopped before the IPL. Verify that they are running correctly and that data integrity is maintained.\\n\\n7. **Documentation\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from pipelines.Learning import SystemOperationPipeline\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = SystemOperationPipeline(client)\n",
    "\n",
    "# Read content\n",
    "with open(\"../assets/examples/learning/operation.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Generate data\n",
    "data = pipeline(content)\n",
    "\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d432400-33aa-4c91-9cbf-d9dc953059cc",
   "metadata": {},
   "source": [
    "### Command References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a1b1015-eacb-4147-a11c-e8fa762ce794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are a programming expert.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Please answer the following question.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"What z/OS command should a system administrator use to list all currently active jobs running on the mainframe for monitoring system performance and resource usage?\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"To list all currently active jobs running on a z/OS mainframe, a system administrator can use the `DA` (Display Active) command within the System Display and Search Facility (SDSF). SDSF is a powerful tool for monitoring and managing jobs, output, and system resources on a z/OS system.\\n\\nHere's how you can use the `DA` command:\\n\\n1. **Access SDSF**: From the ISPF Primary Option Menu, enter `SDSF` or the option number associated with SDSF, and press Enter.\\n\\n2. **Enter the DA Command**: Once in SDSF, type `DA` on the command line and press Enter. This will take you to the \\\"Active Jobs\\\" panel.\\n\\n3. **Review the Active Jobs**: The panel will display a list of all currently active jobs, including TSO users, started tasks, and batch jobs. You can see details such as job name, job ID, owner, CPU time used, and more.\\n\\n4. **Filter and Sort**: You can further filter and sort the list using SDSF commands to focus on specific jobs or performance metrics. For example, you can use the `FILTER` command to narrow down jobs by name or owner, and the `SORT` command to organize the list based on criteria like CPU usage or job ID.\\n\\n5. **Monitor Resources**: By examining the active jobs and their resource usage, you can monitor system performance and identify any jobs that might be consuming excessive resources or causing bottlenecks.\\n\\nUsing the `DA` command in SDSF is a straightforward way for system administrators to keep an eye on the workload and resource utilization on their z/OS mainframe, ensuring efficient operation and performance.\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from pipelines.Learning import CommandReferencePipeline\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = CommandReferencePipeline(client)\n",
    "\n",
    "# Read content\n",
    "with open(\"../assets/examples/learning/reference.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Generate data\n",
    "data = pipeline(content)\n",
    "\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154cb9aa-25f1-4deb-a08f-dbf7d674d587",
   "metadata": {},
   "source": [
    "### Troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e47ca314-ab3e-45f3-974e-4c44fe97ccde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are a programming expert.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Please answer the following question.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"What steps would you take to troubleshoot high CPU usage on an IBM z/OS mainframe system, and how would you utilize tools like RMF and WLM in this process?\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Troubleshooting high CPU usage on an IBM z/OS mainframe system requires a systematic approach, leveraging the available tools like Resource Measurement Facility (RMF) and Workload Manager (WLM). Here\\u2019s a detailed step-by-step process you can follow:\\n\\n### Step 1: Initial Assessment\\n\\n1. **Identify Symptoms**: Begin by identifying the symptoms of high CPU usage. Are there performance complaints from users, batch jobs taking longer, or transaction response times increasing?\\n   \\n2. **Gather Basic Information**: Check the current CPU utilization using basic monitoring tools or system commands to confirm that the CPU usage is indeed high.\\n\\n### Step 2: Utilize RMF for Detailed Analysis\\n\\n1. **Start with RMF Monitor III**: RMF Monitor III provides real-time performance data. Use it to:\\n   - Identify which workloads are consuming the most CPU.\\n   - Check for any unusual spikes in CPU usage.\\n   - Analyze the distribution of CPU usage across different LPARs if you are in a multi-LPAR environment.\\n\\n2. **Review RMF Reports**: Generate RMF reports, especially focusing on:\\n   - **CPU Activity Report**: This report will give you a breakdown of CPU usage by workload and address space.\\n   - **Enqueue Report**: Check for contention issues that might be causing delays and increased CPU usage.\\n   - **Paging Activity Report**: Excessive paging can lead to high CPU usage, so review this to see if it\\u2019s contributing to the problem.\\n\\n### Step 3: Analyze with WLM\\n\\n1. **Review WLM Policies**: Ensure that the WLM policies are set correctly. Incorrect settings might cause certain workloads to consume more CPU than intended.\\n   \\n2. **Check Service Class Goals**: Use WLM to review if the service class goals are being met. If not, it could indicate that workloads are mismanaged, leading to high CPU usage.\\n\\n3. **Adjust WLM Settings**: If necessary, adjust WLM settings to prioritize critical workloads and distribute CPU resources more effectively.\\n\\n### Step 4: Drill Down into Specific Workloads\\n\\n1. **Identify High CPU Consumers**: Use RMF and WLM data to pinpoint specific jobs or transactions that are consuming excessive CPU.\\n   \\n2. **Investigate Problematic Jobs**: Look into the code or logic of these jobs to identify inefficiencies or errors. Consider whether any recent changes might have caused the increase in CPU usage.\\n\\n3. **Check for Loops or Inefficiencies**: Ensure that there are\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from pipelines.Learning import TroubleshootingPipeline\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = TroubleshootingPipeline(client)\n",
    "\n",
    "# Read content\n",
    "with open(\"../assets/examples/learning/troubleshooting.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Generate data\n",
    "data = pipeline(content)\n",
    "\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8988ee29-74be-4c32-adda-dce61a7cf4e3",
   "metadata": {},
   "source": [
    "### Definitions and Terminology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bfbbd51-341a-457f-a64d-ccdf505168a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are a programming expert.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Please answer the following question.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"What are mainframe systems, and in which industries are they typically used for critical applications?\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Mainframe systems are powerful, high-performance computers designed to handle vast amounts of data and support numerous users simultaneously. They are known for their reliability, scalability, and robust processing capabilities, making them ideal for critical applications that require high availability and security.\\n\\nThese systems are typically used in industries where processing large volumes of data quickly and accurately is crucial. Here are some key industries where mainframes play a vital role:\\n\\n1. **Banking and Finance**: Mainframes are the backbone of many financial institutions. They handle transactions, manage accounts, and support online banking services, ensuring data security and processing efficiency for millions of transactions daily.\\n\\n2. **Insurance**: In the insurance industry, mainframes manage policy processing, claims handling, and customer data management, providing the necessary speed and reliability for these complex operations.\\n\\n3. **Healthcare**: Mainframes support electronic health records, patient management systems, and large-scale data analysis, helping healthcare providers deliver efficient and accurate care while maintaining stringent data privacy standards.\\n\\n4. **Retail**: Large retailers use mainframes for inventory management, sales processing, and supply chain operations. They help ensure seamless operations across numerous locations and online platforms.\\n\\n5. **Government**: Government agencies rely on mainframes for processing social security benefits, tax information, and other critical public services. They provide the necessary infrastructure to handle large-scale data processing and maintain data integrity.\\n\\n6. **Telecommunications**: Mainframes manage billing, customer records, and network data, supporting the vast infrastructure required to keep communication networks running smoothly.\\n\\nIn all these industries, the ability of mainframes to process large volumes of data efficiently and securely makes them indispensable for critical applications where downtime or data breaches could have significant repercussions.\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from pipelines.Learning import DefinitionPipeline\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = DefinitionPipeline(client)\n",
    "\n",
    "# Read content\n",
    "with open(\"../assets/examples/learning/definition.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Generate data\n",
    "data = pipeline(content)\n",
    "\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2ae7c0-07fc-4a8b-9b50-654438584fea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
